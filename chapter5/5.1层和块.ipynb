{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a758e597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3004, -0.0015,  0.0696,  0.0112, -0.3014,  0.1669, -0.0074,  0.1977,\n",
       "         -0.1973,  0.1037],\n",
       "        [ 0.2911, -0.0655,  0.1491, -0.0371, -0.2153,  0.0720, -0.0199,  0.1473,\n",
       "         -0.1231, -0.0095]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d16f3",
   "metadata": {},
   "source": [
    "自定义块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e92d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # 用模型参数声明层这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用MLP的父类Module的构造函数来执行必要的初始化\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256) # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)    # 输出层\n",
    "\n",
    "    # 定义模型的前向传播，即如何根据输入x返回所需模型的输出\n",
    "    def forward(self, X):  # nn.Module中实现了__call__方法，在实例被调用时，会自动执行操作并最终调用forward()方法\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad36aa52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1129, -0.0212, -0.0575, -0.1617, -0.0322, -0.2607, -0.2167,  0.0974,\n",
       "         -0.2039,  0.1847],\n",
       "        [-0.0450, -0.0599, -0.1080, -0.1670, -0.0351, -0.1135, -0.1825, -0.0379,\n",
       "         -0.1302,  0.1196]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2968f4f5",
   "metadata": {},
   "source": [
    "顺序块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6162b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例我们把它保存在Module类的成员\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b36aef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2490, -0.0816, -0.0283, -0.0777, -0.3367,  0.2357, -0.0926, -0.0827,\n",
       "          0.0377,  0.3202],\n",
       "        [-0.1729, -0.1149, -0.0616,  0.0312, -0.3250,  0.1550,  0.1133,  0.0240,\n",
       "          0.1642,  0.2296]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c4d85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数，因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量函数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight)+1)\n",
    "        # 复用全连接层这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b14ee65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0631, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b6ca91",
   "metadata": {},
   "source": [
    "可以混合搭配各种组合快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e38f2d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1310, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "    \n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP()) # chimera 假想的怪物\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6414a445",
   "metadata": {},
   "source": [
    "练习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07470c3",
   "metadata": {},
   "source": [
    "将MySequential中存储块的方式更改为Python列表, 可以运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b24204a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0748, -0.1826,  0.0725,  0.1342,  0.2197,  0.0456,  0.0558,  0.1023,\n",
       "         -0.1748, -0.0581],\n",
       "        [ 0.1278, -0.2798, -0.1362,  0.2228,  0.0908,  0.0526, -0.0211,  0.0905,\n",
       "         -0.1827, -0.0699]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential2(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.sequential = []\n",
    "        for module in args:\n",
    "            self.sequential.append(module)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self.sequential:\n",
    "            X = block(X)\n",
    "        return X\n",
    "    \n",
    "net = MySequential2(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbe5ec0",
   "metadata": {},
   "source": [
    "实现平行块，以两个块为参数，如net1, net2, 并返回前向传播中两个网络的串联输出，这也被称为平行块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82494fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.8568e-01,  1.5830e-02, -1.6584e-01, -5.6773e-01,  3.6934e-01,\n",
       "         -3.2824e-01,  3.4321e-01, -1.5514e-01, -8.7827e-02,  3.8812e-01,\n",
       "          1.9380e-01,  4.2359e-01, -1.7288e-01, -9.5152e-01,  1.4269e-01,\n",
       "          4.1053e-02,  9.4714e-02, -1.0936e-02, -5.2025e-01,  1.9197e-01,\n",
       "         -2.4881e-01, -1.3404e-01, -3.6038e-01,  6.5159e-01,  9.4595e-02,\n",
       "          4.0100e-01, -1.5314e-01,  1.6119e-01, -2.9076e-01,  4.7518e-01],\n",
       "        [ 1.9477e-01,  5.8216e-01, -3.2636e-01, -4.7791e-01,  3.1160e-01,\n",
       "         -1.3815e-01,  5.1588e-01, -1.2087e-01, -1.2493e-01,  1.6020e-01,\n",
       "         -3.4580e-02,  3.1538e-01, -4.9339e-02, -6.4446e-01,  1.2129e-01,\n",
       "          3.0762e-02,  2.8115e-01,  2.5723e-01, -2.2428e-01,  2.3390e-01,\n",
       "         -6.3308e-01, -3.3962e-01, -3.1455e-01,  6.7073e-01,  1.4683e-01,\n",
       "          6.6115e-01,  9.5396e-04,  2.4056e-04, -2.8462e-01,  8.1216e-01]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = nn.Sequential(nn.Linear(256, 20))\n",
    "net2 = nn.Sequential(nn.Linear(256, 10))\n",
    "\n",
    "class mergenet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = net1\n",
    "        self.block2 = net2 \n",
    "    \n",
    "    def forward(self, X):\n",
    "        return torch.cat([self.block1(X), self.block2(X)], 1)\n",
    "\n",
    "X = torch.rand(2, 256)\n",
    "net = mergenet()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bd38205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0203, 0.5889, 0.1904, 0.8056, 0.2326, 0.9335, 0.3181, 0.2885, 0.4204,\n",
       "         0.2408, 0.9753, 0.3264, 0.4023, 0.3978, 0.7310, 0.4318, 0.1090, 0.6453,\n",
       "         0.7487, 0.2230, 0.9060, 0.8040, 0.0374, 0.0418, 0.9539, 0.2429, 0.6423,\n",
       "         0.3163, 0.5829, 0.5764, 0.6383, 0.4113, 0.0980, 0.2311, 0.8769, 0.0660,\n",
       "         0.7674, 0.2595, 0.5674, 0.6688, 0.8120, 0.5547, 0.3177, 0.3633, 0.9289,\n",
       "         0.5007, 0.0990, 0.1641, 0.4705, 0.1240, 0.9538, 0.2681, 0.5460, 0.0818,\n",
       "         0.4137, 0.6361, 0.5267, 0.5399, 0.6742, 0.2943, 0.9308, 0.9509, 0.3483,\n",
       "         0.5792, 0.5896, 0.8851, 0.2023, 0.1578, 0.0939, 0.9470, 0.1008, 0.0281,\n",
       "         0.9477, 0.1121, 0.8407, 0.6089, 0.9535, 0.5369, 0.4287, 0.7420, 0.6949,\n",
       "         0.9552, 0.6196, 0.6838, 0.5749, 0.8095, 0.6305, 0.1335, 0.9491, 0.4728,\n",
       "         0.9970, 0.7911, 0.5351, 0.7858, 0.2103, 0.8462, 0.2459, 0.5608, 0.2128,\n",
       "         0.2208, 0.6296, 0.9951, 0.0032, 0.6167, 0.1951, 0.7064, 0.7845, 0.7224,\n",
       "         0.3729, 0.3789, 0.1144, 0.6721, 0.5921, 0.4077, 0.8744, 0.6486, 0.1017,\n",
       "         0.2755, 0.1025, 0.9205, 0.0603, 0.7881, 0.1750, 0.3397, 0.3402, 0.7228,\n",
       "         0.0199, 0.3745, 0.8972, 0.9232, 0.8347, 0.1483, 0.4999, 0.3681, 0.8209,\n",
       "         0.5321, 0.9566, 0.2017, 0.3864, 0.0011, 0.8325, 0.3594, 0.0073, 0.9539,\n",
       "         0.1682, 0.1777, 0.9063, 0.8645, 0.6414, 0.3016, 0.2688, 0.7558, 0.1114,\n",
       "         0.2339, 0.2252, 0.8656, 0.5713, 0.7026, 0.5947, 0.4652, 0.9269, 0.8922,\n",
       "         0.7114, 0.1518, 0.7380, 0.7449, 0.5601, 0.8577, 0.3201, 0.8660, 0.3185,\n",
       "         0.2912, 0.2083, 0.6032, 0.5188, 0.9724, 0.5171, 0.5090, 0.4819, 0.6549,\n",
       "         0.9047, 0.6087, 0.9880, 0.3850, 0.0827, 0.3148, 0.6373, 0.1598, 0.7479,\n",
       "         0.6084, 0.9208, 0.9768, 0.8198, 0.0612, 0.7723, 0.4552, 0.5581, 0.8145,\n",
       "         0.0914, 0.8390, 0.4090, 0.8832, 0.5959, 0.1949, 0.4316, 0.6870, 0.0757,\n",
       "         0.8977, 0.1308, 0.3986, 0.6941, 0.9697, 0.7192, 0.0732, 0.8498, 0.5049,\n",
       "         0.2876, 0.3069, 0.0307, 0.5208, 0.5492, 0.5504, 0.0537, 0.7082, 0.2599,\n",
       "         0.1010, 0.3303, 0.7818, 0.4583, 0.1892, 0.6020, 0.6593, 0.2716, 0.0953,\n",
       "         0.4725, 0.1772, 0.1441, 0.9618, 0.5552, 0.0076, 0.2431, 0.2611, 0.9580,\n",
       "         0.1243, 0.3727, 0.6336, 0.9211, 0.5395, 0.5412, 0.8218, 0.6119, 0.5580,\n",
       "         0.7989, 0.7086, 0.8904, 0.0744],\n",
       "        [0.5815, 0.9750, 0.7388, 0.3158, 0.3457, 0.3327, 0.2356, 0.3924, 0.1528,\n",
       "         0.6745, 0.2317, 0.5447, 0.2532, 0.3096, 0.4702, 0.7498, 0.3636, 0.1690,\n",
       "         0.6318, 0.5946, 0.6208, 0.1039, 0.1423, 0.4966, 0.4458, 0.9798, 0.3756,\n",
       "         0.0336, 0.5675, 0.1977, 0.5770, 0.9205, 0.7076, 0.3520, 0.2948, 0.6301,\n",
       "         0.6895, 0.4929, 0.9110, 0.0100, 0.4601, 0.2086, 0.0847, 0.6150, 0.0874,\n",
       "         0.6875, 0.4695, 0.1912, 0.2236, 0.0179, 0.6163, 0.0354, 0.5217, 0.5041,\n",
       "         0.9028, 0.8381, 0.6518, 0.7369, 0.1709, 0.0470, 0.9829, 0.4235, 0.0704,\n",
       "         0.3010, 0.4290, 0.0662, 0.3649, 0.0692, 0.1693, 0.8657, 0.1497, 0.8431,\n",
       "         0.5350, 0.1686, 0.2166, 0.5382, 0.9614, 0.5037, 0.0079, 0.9832, 0.3916,\n",
       "         0.9669, 0.7399, 0.3103, 0.8338, 0.9608, 0.2077, 0.7361, 0.7361, 0.3615,\n",
       "         0.7005, 0.9877, 0.3519, 0.9996, 0.9487, 0.0808, 0.2712, 0.2776, 0.6389,\n",
       "         0.5338, 0.4674, 0.7520, 0.4268, 0.9964, 0.2766, 0.6445, 0.5711, 0.1086,\n",
       "         0.2696, 0.1863, 0.6730, 0.8726, 0.9278, 0.3996, 0.4652, 0.5133, 0.1752,\n",
       "         0.8660, 0.0977, 0.3882, 0.8494, 0.8637, 0.7151, 0.5991, 0.3205, 0.8188,\n",
       "         0.3169, 0.9724, 0.3080, 0.4454, 0.7669, 0.6183, 0.2883, 0.0977, 0.6525,\n",
       "         0.2061, 0.9072, 0.4018, 0.0422, 0.9864, 0.7328, 0.0960, 0.0863, 0.0841,\n",
       "         0.9874, 0.1637, 0.2893, 0.7226, 0.5364, 0.4417, 0.5680, 0.3202, 0.1306,\n",
       "         0.3745, 0.3109, 0.6303, 0.5320, 0.1978, 0.9833, 0.7356, 0.8250, 0.8492,\n",
       "         0.0733, 0.5082, 0.0697, 0.3455, 0.7644, 0.1547, 0.1717, 0.8559, 0.1840,\n",
       "         0.5213, 0.1850, 0.2320, 0.2790, 0.9315, 0.3151, 0.9223, 0.1059, 0.0767,\n",
       "         0.1068, 0.2343, 0.0266, 0.9290, 0.8540, 0.9313, 0.1587, 0.4490, 0.6382,\n",
       "         0.8370, 0.9447, 0.4052, 0.9667, 0.3430, 0.8223, 0.2805, 0.9433, 0.9404,\n",
       "         0.7145, 0.1716, 0.3400, 0.4098, 0.2389, 0.8256, 0.6775, 0.7745, 0.9013,\n",
       "         0.1924, 0.1367, 0.5437, 0.9213, 0.9346, 0.1919, 0.9425, 0.1229, 0.3716,\n",
       "         0.7122, 0.2222, 0.4313, 0.7342, 0.0519, 0.9159, 0.1311, 0.4871, 0.1865,\n",
       "         0.6184, 0.3282, 0.2321, 0.7269, 0.7674, 0.6267, 0.8930, 0.3372, 0.2972,\n",
       "         0.3517, 0.5030, 0.8147, 0.7083, 0.6567, 0.0365, 0.2940, 0.3314, 0.0550,\n",
       "         0.5680, 0.4262, 0.7257, 0.9934, 0.7042, 0.6709, 0.1322, 0.4577, 0.4768,\n",
       "         0.6206, 0.4777, 0.9938, 0.7969]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2, 256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
